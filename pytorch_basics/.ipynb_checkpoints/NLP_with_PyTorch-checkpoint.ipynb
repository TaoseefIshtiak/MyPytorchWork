{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c5zUX45c7ll7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dip9L8KW7-yq"
   },
   "outputs": [],
   "source": [
    "with open('/content/shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BUj65aaP8z2B",
    "outputId": "943dadc2-4697-4d27-c307-f48cd9fe6577"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "id": "MnGohlXL8FlG",
    "outputId": "3b3f79f2-a875-426b-984d-93963e6c6537"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sFKVCyDaC7tt",
    "outputId": "2705532c-f4e1-475f-bef2-f4922e2804a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) #no of characters in the entire text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCT1IGBJDDO2"
   },
   "outputs": [],
   "source": [
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YvDp7-psDFAB",
    "outputId": "1921d374-4202-40f0-bad5-7900d8ec992e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8HJPaJ1gDJre",
    "outputId": "06fae13f-74e2-414b-81c0-85a67340e311"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'v')\n",
      "(1, '|')\n",
      "(2, 'V')\n",
      "(3, 'r')\n",
      "(4, 'z')\n",
      "(5, ':')\n",
      "(6, 'R')\n",
      "(7, '`')\n",
      "(8, 'M')\n",
      "(9, 'W')\n",
      "(10, ';')\n",
      "(11, '<')\n",
      "(12, 'J')\n",
      "(13, ' ')\n",
      "(14, 'T')\n",
      "(15, 'A')\n",
      "(16, 'E')\n",
      "(17, '!')\n",
      "(18, 'L')\n",
      "(19, '7')\n",
      "(20, 'y')\n",
      "(21, 'O')\n",
      "(22, 'a')\n",
      "(23, '9')\n",
      "(24, '8')\n",
      "(25, '[')\n",
      "(26, '0')\n",
      "(27, 'P')\n",
      "(28, 'D')\n",
      "(29, 'j')\n",
      "(30, 'Z')\n",
      "(31, 'e')\n",
      "(32, 'b')\n",
      "(33, 'Q')\n",
      "(34, '-')\n",
      "(35, ',')\n",
      "(36, 's')\n",
      "(37, 'p')\n",
      "(38, 'l')\n",
      "(39, 'u')\n",
      "(40, 'S')\n",
      "(41, 'F')\n",
      "(42, 'G')\n",
      "(43, '2')\n",
      "(44, '3')\n",
      "(45, '&')\n",
      "(46, '?')\n",
      "(47, '5')\n",
      "(48, 'i')\n",
      "(49, 'x')\n",
      "(50, '6')\n",
      "(51, 'q')\n",
      "(52, 't')\n",
      "(53, 'n')\n",
      "(54, 'B')\n",
      "(55, 'N')\n",
      "(56, 'H')\n",
      "(57, ')')\n",
      "(58, 'K')\n",
      "(59, \"'\")\n",
      "(60, '1')\n",
      "(61, ']')\n",
      "(62, 'm')\n",
      "(63, 'd')\n",
      "(64, 'o')\n",
      "(65, 'k')\n",
      "(66, '.')\n",
      "(67, 'h')\n",
      "(68, 'c')\n",
      "(69, 'f')\n",
      "(70, 'C')\n",
      "(71, 'w')\n",
      "(72, '4')\n",
      "(73, 'U')\n",
      "(74, '(')\n",
      "(75, 'g')\n",
      "(76, '_')\n",
      "(77, '>')\n",
      "(78, '}')\n",
      "(79, 'X')\n",
      "(80, '\"')\n",
      "(81, 'Y')\n",
      "(82, '\\n')\n",
      "(83, 'I')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(all_characters):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tXZOuR7ZDLjK"
   },
   "outputs": [],
   "source": [
    "decoder = dict(enumerate(all_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8J5yzj2kDOTy",
    "outputId": "e5ea0af3-8bd7-4a49-db57-e55b1ca61087"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'v',\n",
       " 1: '|',\n",
       " 2: 'V',\n",
       " 3: 'r',\n",
       " 4: 'z',\n",
       " 5: ':',\n",
       " 6: 'R',\n",
       " 7: '`',\n",
       " 8: 'M',\n",
       " 9: 'W',\n",
       " 10: ';',\n",
       " 11: '<',\n",
       " 12: 'J',\n",
       " 13: ' ',\n",
       " 14: 'T',\n",
       " 15: 'A',\n",
       " 16: 'E',\n",
       " 17: '!',\n",
       " 18: 'L',\n",
       " 19: '7',\n",
       " 20: 'y',\n",
       " 21: 'O',\n",
       " 22: 'a',\n",
       " 23: '9',\n",
       " 24: '8',\n",
       " 25: '[',\n",
       " 26: '0',\n",
       " 27: 'P',\n",
       " 28: 'D',\n",
       " 29: 'j',\n",
       " 30: 'Z',\n",
       " 31: 'e',\n",
       " 32: 'b',\n",
       " 33: 'Q',\n",
       " 34: '-',\n",
       " 35: ',',\n",
       " 36: 's',\n",
       " 37: 'p',\n",
       " 38: 'l',\n",
       " 39: 'u',\n",
       " 40: 'S',\n",
       " 41: 'F',\n",
       " 42: 'G',\n",
       " 43: '2',\n",
       " 44: '3',\n",
       " 45: '&',\n",
       " 46: '?',\n",
       " 47: '5',\n",
       " 48: 'i',\n",
       " 49: 'x',\n",
       " 50: '6',\n",
       " 51: 'q',\n",
       " 52: 't',\n",
       " 53: 'n',\n",
       " 54: 'B',\n",
       " 55: 'N',\n",
       " 56: 'H',\n",
       " 57: ')',\n",
       " 58: 'K',\n",
       " 59: \"'\",\n",
       " 60: '1',\n",
       " 61: ']',\n",
       " 62: 'm',\n",
       " 63: 'd',\n",
       " 64: 'o',\n",
       " 65: 'k',\n",
       " 66: '.',\n",
       " 67: 'h',\n",
       " 68: 'c',\n",
       " 69: 'f',\n",
       " 70: 'C',\n",
       " 71: 'w',\n",
       " 72: '4',\n",
       " 73: 'U',\n",
       " 74: '(',\n",
       " 75: 'g',\n",
       " 76: '_',\n",
       " 77: '>',\n",
       " 78: '}',\n",
       " 79: 'X',\n",
       " 80: '\"',\n",
       " 81: 'Y',\n",
       " 82: '\\n',\n",
       " 83: 'I'}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "cuT_oPazDQ4j",
    "outputId": "a17c8376-249d-415d-9501-d093fc6286b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(0, 'v'), (1, '|'), (2, 'V'), (3, 'r'), (4, 'z'), (5, ':'), (6, 'R'), (7, '`'), (8, 'M'), (9, 'W'), (10, ';'), (11, '<'), (12, 'J'), (13, ' '), (14, 'T'), (15, 'A'), (16, 'E'), (17, '!'), (18, 'L'), (19, '7'), (20, 'y'), (21, 'O'), (22, 'a'), (23, '9'), (24, '8'), (25, '['), (26, '0'), (27, 'P'), (28, 'D'), (29, 'j'), (30, 'Z'), (31, 'e'), (32, 'b'), (33, 'Q'), (34, '-'), (35, ','), (36, 's'), (37, 'p'), (38, 'l'), (39, 'u'), (40, 'S'), (41, 'F'), (42, 'G'), (43, '2'), (44, '3'), (45, '&'), (46, '?'), (47, '5'), (48, 'i'), (49, 'x'), (50, '6'), (51, 'q'), (52, 't'), (53, 'n'), (54, 'B'), (55, 'N'), (56, 'H'), (57, ')'), (58, 'K'), (59, \"'\"), (60, '1'), (61, ']'), (62, 'm'), (63, 'd'), (64, 'o'), (65, 'k'), (66, '.'), (67, 'h'), (68, 'c'), (69, 'f'), (70, 'C'), (71, 'w'), (72, '4'), (73, 'U'), (74, '('), (75, 'g'), (76, '_'), (77, '>'), (78, '}'), (79, 'X'), (80, '\"'), (81, 'Y'), (82, '\\n'), (83, 'I')])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JP0qMOvFDTNe"
   },
   "outputs": [],
   "source": [
    "encoder = {char: ind for ind,char in decoder.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u0dyQNl5DWCN",
    "outputId": "3953f052-8fa7-4a37-a144-3fc038f4b6ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 82,\n",
       " ' ': 13,\n",
       " '!': 17,\n",
       " '\"': 80,\n",
       " '&': 45,\n",
       " \"'\": 59,\n",
       " '(': 74,\n",
       " ')': 57,\n",
       " ',': 35,\n",
       " '-': 34,\n",
       " '.': 66,\n",
       " '0': 26,\n",
       " '1': 60,\n",
       " '2': 43,\n",
       " '3': 44,\n",
       " '4': 72,\n",
       " '5': 47,\n",
       " '6': 50,\n",
       " '7': 19,\n",
       " '8': 24,\n",
       " '9': 23,\n",
       " ':': 5,\n",
       " ';': 10,\n",
       " '<': 11,\n",
       " '>': 77,\n",
       " '?': 46,\n",
       " 'A': 15,\n",
       " 'B': 54,\n",
       " 'C': 70,\n",
       " 'D': 28,\n",
       " 'E': 16,\n",
       " 'F': 41,\n",
       " 'G': 42,\n",
       " 'H': 56,\n",
       " 'I': 83,\n",
       " 'J': 12,\n",
       " 'K': 58,\n",
       " 'L': 18,\n",
       " 'M': 8,\n",
       " 'N': 55,\n",
       " 'O': 21,\n",
       " 'P': 27,\n",
       " 'Q': 33,\n",
       " 'R': 6,\n",
       " 'S': 40,\n",
       " 'T': 14,\n",
       " 'U': 73,\n",
       " 'V': 2,\n",
       " 'W': 9,\n",
       " 'X': 79,\n",
       " 'Y': 81,\n",
       " 'Z': 30,\n",
       " '[': 25,\n",
       " ']': 61,\n",
       " '_': 76,\n",
       " '`': 7,\n",
       " 'a': 22,\n",
       " 'b': 32,\n",
       " 'c': 68,\n",
       " 'd': 63,\n",
       " 'e': 31,\n",
       " 'f': 69,\n",
       " 'g': 75,\n",
       " 'h': 67,\n",
       " 'i': 48,\n",
       " 'j': 29,\n",
       " 'k': 65,\n",
       " 'l': 38,\n",
       " 'm': 62,\n",
       " 'n': 53,\n",
       " 'o': 64,\n",
       " 'p': 37,\n",
       " 'q': 51,\n",
       " 'r': 3,\n",
       " 's': 36,\n",
       " 't': 52,\n",
       " 'u': 39,\n",
       " 'v': 0,\n",
       " 'w': 71,\n",
       " 'x': 49,\n",
       " 'y': 20,\n",
       " 'z': 4,\n",
       " '|': 1,\n",
       " '}': 78}"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXVF-8hUDXv_"
   },
   "outputs": [],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "lKFD9NKzDaRK",
    "outputId": "dc87504f-0e3e-4db0-fb0c-fca4b6bf0460"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "       13, 13, 13, 13, 13, 60, 82, 13, 13, 41,  3, 64, 62, 13, 69, 22, 48,\n",
       "        3, 31, 36, 52, 13, 68,  3, 31, 22, 52, 39,  3, 31, 36, 13, 71, 31,\n",
       "       13, 63, 31, 36, 48,  3, 31, 13, 48, 53, 68,  3, 31, 22, 36, 31, 35,\n",
       "       82, 13, 13, 14, 67, 22, 52, 13, 52, 67, 31,  3, 31, 32, 20, 13, 32,\n",
       "       31, 22, 39, 52, 20, 59, 36, 13,  3, 64, 36, 31, 13, 62, 48, 75, 67,\n",
       "       52, 13, 53, 31,  0, 31,  3, 13, 63, 48, 31, 35, 82, 13, 13, 54, 39,\n",
       "       52, 13, 22, 36, 13, 52, 67, 31, 13,  3, 48, 37, 31,  3, 13, 36, 67,\n",
       "       64, 39, 38, 63, 13, 32, 20, 13, 52, 48, 62, 31, 13, 63, 31, 68, 31,\n",
       "       22, 36, 31, 35, 82, 13, 13, 56, 48, 36, 13, 52, 31, 53, 63, 31,  3,\n",
       "       13, 67, 31, 48,  3, 13, 62, 48, 75, 67, 52, 13, 32, 31, 22,  3, 13,\n",
       "       67, 48, 36, 13, 62, 31, 62, 64,  3, 20,  5, 82, 13, 13, 54, 39, 52,\n",
       "       13, 52, 67, 64, 39, 13, 68, 64, 53, 52,  3, 22, 68, 52, 31, 63, 13,\n",
       "       52, 64, 13, 52, 67, 48, 53, 31, 13, 64, 71, 53, 13, 32,  3, 48, 75,\n",
       "       67, 52, 13, 31, 20, 31, 36, 35, 82, 13, 13, 41, 31, 31, 63, 59, 36,\n",
       "       52, 13, 52, 67, 20, 13, 38, 48, 75, 67, 52, 59, 36, 13, 69, 38, 22,\n",
       "       62, 31, 13, 71, 48, 52, 67, 13, 36, 31, 38, 69, 34, 36, 39, 32, 36,\n",
       "       52, 22, 53, 52, 48, 22, 38, 13, 69, 39, 31, 38, 35, 82, 13, 13,  8,\n",
       "       22, 65, 48, 53, 75, 13, 22, 13, 69, 22, 62, 48, 53, 31, 13, 71, 67,\n",
       "       31,  3, 31, 13, 22, 32, 39, 53, 63, 22, 53, 68, 31, 13, 38, 48, 31,\n",
       "       36, 35, 82, 13, 13, 14, 67, 20, 13, 36, 31, 38, 69, 13, 52, 67, 20,\n",
       "       13, 69, 64, 31, 35, 13, 52, 64, 13, 52, 67, 20, 13, 36, 71, 31, 31,\n",
       "       52, 13, 36, 31, 38, 69, 13, 52, 64, 64, 13, 68,  3, 39, 31, 38,  5,\n",
       "       82, 13, 13, 14, 67, 64, 39, 13, 52, 67, 22, 52, 13, 22,  3, 52, 13,\n",
       "       53, 64, 71, 13, 52, 67, 31, 13, 71, 64,  3, 38, 63, 59, 36, 13, 69,\n",
       "        3, 31, 36, 67, 13, 64,  3, 53, 22, 62, 31, 53, 52, 35, 82, 13, 13,\n",
       "       15, 53, 63, 13, 64, 53, 38, 20, 13, 67, 31,  3, 22, 38, 63, 13, 52,\n",
       "       64, 13, 52, 67, 31, 13, 75, 22, 39, 63, 20, 13, 36, 37,  3, 48, 53,\n",
       "       75, 35, 82, 13, 13,  9, 48, 52, 67, 48, 53, 13, 52, 67, 48, 53, 31,\n",
       "       13, 64, 71, 53, 13, 32, 39])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZU5lL8nkDgIv"
   },
   "outputs": [],
   "source": [
    "def one_hot_encoder(encoded_text, num_uni_chars):\n",
    "    '''\n",
    "    encoded_text : batch of encoded text\n",
    "    \n",
    "    num_uni_chars = number of unique characters (len(set(text)))\n",
    "    '''\n",
    "    \n",
    "    # method obtained from:\n",
    "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
    "      \n",
    "    # creating a placeholder for zeros.\n",
    "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "    \n",
    "    # converting data type for later use with pytorch (errors if we dont!)\n",
    "    one_hot = one_hot.astype(np.float32)\n",
    "\n",
    "    # using fancy indexing fill in the 1s at the correct index locations\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "    \n",
    "\n",
    "    # reshaping it so it matches the batch sahe\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B44kB9IIDi_o"
   },
   "outputs": [],
   "source": [
    "#small example of one hot encoding\n",
    "arr = np.array([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "9cZivY1MDlik",
    "outputId": "89d80adf-29aa-4009-ab3e-3160b24a07e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "96Ros5CDDnDr",
    "outputId": "f3c4de8b-1c4f-4352-9386-2f8325a7117b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoder(arr, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RFWtbH9pDs-S"
   },
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
    "    \n",
    "    '''\n",
    "    Generating (using yield) batches for training.\n",
    "    \n",
    "    X: Encoded Text of length seq_len\n",
    "    Y: Encoded Text shifted by one\n",
    "    Example:\n",
    "    X:\n",
    "    [[1 2 3]]\n",
    "    \n",
    "    Y:\n",
    "    [[ 2 3 4]]\n",
    "    encoded_text : Completing Encoded Text to make batches from batch_size : Number of samples per batch seq_len : \n",
    "    Length of character sequence\n",
    "       \n",
    "    '''\n",
    "    # Total number of characters per batch\n",
    "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100  characters come out per batch.\n",
    "    char_per_batch = samp_per_batch * seq_len\n",
    "    \n",
    "    \n",
    "    # Number of batches available to make\n",
    "    # using int() to roun to nearest integer\n",
    "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
    "    \n",
    "    # cutting off end of encoded_text that\n",
    "    # won't fit evenly into a batch\n",
    "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
    "    \n",
    "    \n",
    "    # Reshape text into rows the size of a batch\n",
    "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
    "    \n",
    "\n",
    "    # Go through each row in array.\n",
    "    for n in range(0, encoded_text.shape[1], seq_len):\n",
    "        \n",
    "        # Grab feature characters\n",
    "        x = encoded_text[:, n:n+seq_len]\n",
    "        \n",
    "        # y is the target shifted over by 1\n",
    "        y = np.zeros_like(x)\n",
    "       \n",
    "        #\n",
    "        try:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
    "            \n",
    "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
    "        except:\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KfjSMAzEDwlq"
   },
   "outputs": [],
   "source": [
    "sample_text = encoded_text[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qkUO4suHDy46",
    "outputId": "1c121e0b-8cb9-41db-cb14-da17affa197c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([82, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13,\n",
       "       13, 13, 13])"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMn32rTED09_"
   },
   "outputs": [],
   "source": [
    "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KuFmW9r_D3Zb"
   },
   "outputs": [],
   "source": [
    "x, y  = next(batch_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cuWk_YkxD59s",
    "outputId": "80536270-688e-4c96-e169-a32dd4bbf46c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[82, 13, 13, 13, 13],\n",
       "       [13, 13, 13, 13, 13]])"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8_WrIy_PD8pi",
    "outputId": "2296d5b4-031a-4287-90de-099997adf776"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13, 13, 13, 13, 13],\n",
       "       [13, 13, 13, 13, 13]])"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XgY_EYpiD_Xh",
    "outputId": "f1a8ad23-7baf-4318-a119-256d4982b785"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ErHYcFk9KYKK"
   },
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vfZH9AsEKbZS"
   },
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
    "        \n",
    "        \n",
    "        # setting up attributes\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        #CHARACTER SET, ENCODER, and DECODER\n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = dict(enumerate(all_chars))\n",
    "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
    "        \n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "                  \n",
    "        \n",
    "        lstm_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        \n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        \n",
    "        \n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        '''\n",
    "        Used as separate method to account for both GPU and CPU users.\n",
    "        '''\n",
    "        \n",
    "        if self.use_gpu:\n",
    "            \n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jIm-URP0N6Qw"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CharModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e3a8d6691641>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model = CharModel(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mall_chars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mall_characters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnum_hidden\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnum_layers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdrop_prob\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CharModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fDSp_xwYOAMZ"
   },
   "outputs": [],
   "source": [
    "total_param  = []\n",
    "for p in model.parameters():\n",
    "    total_param.append(int(p.numel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "kKVsmlXqOhNt",
    "outputId": "5d28a525-00b1-451f-9350-f1c7fca432f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YnNLUI0VOjfk",
    "outputId": "3a97991f-4316-4ce5-8041-2826499b1151"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5445609"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUYiANhbOlme"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmu6OngUOn30"
   },
   "outputs": [],
   "source": [
    "# percentage of data to be used for training\n",
    "train_percent = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zCXZSpH9Opg-",
    "outputId": "6d0d9ff4-f769-408e-e825-d7fa7848fa59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "544560"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "74RPqFovOsO9"
   },
   "outputs": [],
   "source": [
    "train_ind = int(len(encoded_text) * (train_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cz6Pd6bKOuH6"
   },
   "outputs": [],
   "source": [
    "train_data = encoded_text[:train_ind]\n",
    "val_data = encoded_text[train_ind:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyW-qnPGOwqW"
   },
   "outputs": [],
   "source": [
    "# Epochs to train for\n",
    "epochs = 50\n",
    "# batch size \n",
    "batch_size = 128\n",
    "\n",
    "# Length of sequence\n",
    "seq_len = 100\n",
    "\n",
    "# for printing report purposes\n",
    "# always start at 0\n",
    "tracker = 0\n",
    "\n",
    "# number of characters in text\n",
    "num_char = max(encoded_text)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "g8yaKrmMO8yW",
    "outputId": "9a282b49-5940-438b-aa06-71129e1b15d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Step: 25 Val Loss: 3.236757278442383\n",
      "Epoch: 1 Step: 50 Val Loss: 3.23431396484375\n",
      "Epoch: 1 Step: 75 Val Loss: 3.219740867614746\n",
      "Epoch: 2 Step: 100 Val Loss: 3.081172227859497\n",
      "Epoch: 2 Step: 125 Val Loss: 3.0007073879241943\n",
      "Epoch: 3 Step: 150 Val Loss: 2.871270179748535\n",
      "Epoch: 4 Step: 175 Val Loss: 2.7627992630004883\n",
      "Epoch: 4 Step: 200 Val Loss: 2.7278223037719727\n",
      "Epoch: 5 Step: 225 Val Loss: 2.585216760635376\n",
      "Epoch: 5 Step: 250 Val Loss: 2.4824752807617188\n",
      "Epoch: 6 Step: 275 Val Loss: 2.4163308143615723\n",
      "Epoch: 7 Step: 300 Val Loss: 2.332632064819336\n",
      "Epoch: 7 Step: 325 Val Loss: 2.2637763023376465\n",
      "Epoch: 8 Step: 350 Val Loss: 2.2015116214752197\n",
      "Epoch: 8 Step: 375 Val Loss: 2.158773183822632\n",
      "Epoch: 9 Step: 400 Val Loss: 2.122689723968506\n",
      "Epoch: 10 Step: 425 Val Loss: 2.092031240463257\n",
      "Epoch: 10 Step: 450 Val Loss: 2.0649685859680176\n",
      "Epoch: 11 Step: 475 Val Loss: 2.0459237098693848\n",
      "Epoch: 11 Step: 500 Val Loss: 2.0171940326690674\n",
      "Epoch: 12 Step: 525 Val Loss: 1.9984043836593628\n",
      "Epoch: 13 Step: 550 Val Loss: 1.9892683029174805\n",
      "Epoch: 13 Step: 575 Val Loss: 1.9635120630264282\n",
      "Epoch: 14 Step: 600 Val Loss: 1.9521199464797974\n",
      "Epoch: 14 Step: 625 Val Loss: 1.9375766515731812\n",
      "Epoch: 15 Step: 650 Val Loss: 1.9192689657211304\n",
      "Epoch: 16 Step: 675 Val Loss: 1.9048384428024292\n",
      "Epoch: 16 Step: 700 Val Loss: 1.8978112936019897\n",
      "Epoch: 17 Step: 725 Val Loss: 1.8885154724121094\n",
      "Epoch: 17 Step: 750 Val Loss: 1.8822410106658936\n",
      "Epoch: 18 Step: 775 Val Loss: 1.874367117881775\n",
      "Epoch: 19 Step: 800 Val Loss: 1.8649729490280151\n",
      "Epoch: 19 Step: 825 Val Loss: 1.8544901609420776\n",
      "Epoch: 20 Step: 850 Val Loss: 1.851454734802246\n",
      "Epoch: 20 Step: 875 Val Loss: 1.8400779962539673\n",
      "Epoch: 21 Step: 900 Val Loss: 1.8371222019195557\n",
      "Epoch: 22 Step: 925 Val Loss: 1.8254516124725342\n",
      "Epoch: 22 Step: 950 Val Loss: 1.8248493671417236\n",
      "Epoch: 23 Step: 975 Val Loss: 1.8204891681671143\n",
      "Epoch: 23 Step: 1000 Val Loss: 1.8112990856170654\n",
      "Epoch: 24 Step: 1025 Val Loss: 1.810265064239502\n",
      "Epoch: 24 Step: 1050 Val Loss: 1.8055270910263062\n",
      "Epoch: 25 Step: 1075 Val Loss: 1.799338936805725\n",
      "Epoch: 26 Step: 1100 Val Loss: 1.7884912490844727\n",
      "Epoch: 26 Step: 1125 Val Loss: 1.786313772201538\n",
      "Epoch: 27 Step: 1150 Val Loss: 1.786668062210083\n",
      "Epoch: 27 Step: 1175 Val Loss: 1.7896519899368286\n",
      "Epoch: 28 Step: 1200 Val Loss: 1.7781566381454468\n",
      "Epoch: 29 Step: 1225 Val Loss: 1.7734817266464233\n",
      "Epoch: 29 Step: 1250 Val Loss: 1.7690937519073486\n",
      "Epoch: 30 Step: 1275 Val Loss: 1.7662663459777832\n",
      "Epoch: 30 Step: 1300 Val Loss: 1.7691128253936768\n",
      "Epoch: 31 Step: 1325 Val Loss: 1.7614566087722778\n",
      "Epoch: 32 Step: 1350 Val Loss: 1.7573570013046265\n",
      "Epoch: 32 Step: 1375 Val Loss: 1.7512954473495483\n",
      "Epoch: 33 Step: 1400 Val Loss: 1.750986933708191\n",
      "Epoch: 33 Step: 1425 Val Loss: 1.7450147867202759\n",
      "Epoch: 34 Step: 1450 Val Loss: 1.7530834674835205\n",
      "Epoch: 35 Step: 1475 Val Loss: 1.7444496154785156\n",
      "Epoch: 35 Step: 1500 Val Loss: 1.7377008199691772\n",
      "Epoch: 36 Step: 1525 Val Loss: 1.744083285331726\n",
      "Epoch: 36 Step: 1550 Val Loss: 1.74072265625\n",
      "Epoch: 37 Step: 1575 Val Loss: 1.7397443056106567\n",
      "Epoch: 38 Step: 1600 Val Loss: 1.7357656955718994\n",
      "Epoch: 38 Step: 1625 Val Loss: 1.7284741401672363\n",
      "Epoch: 39 Step: 1650 Val Loss: 1.7411181926727295\n",
      "Epoch: 39 Step: 1675 Val Loss: 1.7338846921920776\n",
      "Epoch: 40 Step: 1700 Val Loss: 1.7390729188919067\n",
      "Epoch: 41 Step: 1725 Val Loss: 1.7412972450256348\n",
      "Epoch: 41 Step: 1750 Val Loss: 1.732897162437439\n",
      "Epoch: 42 Step: 1775 Val Loss: 1.7372026443481445\n",
      "Epoch: 42 Step: 1800 Val Loss: 1.7338088750839233\n",
      "Epoch: 43 Step: 1825 Val Loss: 1.7371931076049805\n",
      "Epoch: 44 Step: 1850 Val Loss: 1.7341668605804443\n",
      "Epoch: 44 Step: 1875 Val Loss: 1.7420966625213623\n",
      "Epoch: 45 Step: 1900 Val Loss: 1.732323408126831\n",
      "Epoch: 45 Step: 1925 Val Loss: 1.7305337190628052\n",
      "Epoch: 46 Step: 1950 Val Loss: 1.7359895706176758\n",
      "Epoch: 47 Step: 1975 Val Loss: 1.7407742738723755\n",
      "Epoch: 47 Step: 2000 Val Loss: 1.7422230243682861\n",
      "Epoch: 48 Step: 2025 Val Loss: 1.7410410642623901\n",
      "Epoch: 48 Step: 2050 Val Loss: 1.7271350622177124\n",
      "Epoch: 49 Step: 2075 Val Loss: 1.738735318183899\n",
      "Epoch: 49 Step: 2100 Val Loss: 1.7415802478790283\n"
     ]
    }
   ],
   "source": [
    "# Set model to train\n",
    "model.train()\n",
    "\n",
    "\n",
    "# Check to see if using GPU\n",
    "if model.use_gpu:\n",
    "    model.cuda()\n",
    "\n",
    "for i in range(epochs):\n",
    "    \n",
    "    hidden = model.hidden_state(batch_size)\n",
    "    \n",
    "    \n",
    "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
    "        \n",
    "        tracker += 1\n",
    "        \n",
    "        # One Hot Encode incoming data\n",
    "        x = one_hot_encoder(x,num_char)\n",
    "        \n",
    "        # Convert Numpy Arrays to Tensor\n",
    "        \n",
    "        inputs = torch.from_numpy(x)\n",
    "        targets = torch.from_numpy(y)\n",
    "        \n",
    "        # Adjust for GPU if necessary\n",
    "        \n",
    "        if model.use_gpu:\n",
    "            \n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Reset Hidden State\n",
    "        # If we dont' reset we would backpropagate through all training history\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        lstm_output, hidden = model.forward(inputs,hidden)\n",
    "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
    "        # LET\"S CLIP JUST IN CASE\n",
    "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ###################################\n",
    "        ### CHECK ON VALIDATION SET ######\n",
    "        #################################\n",
    "        \n",
    "        if tracker % 25 == 0:\n",
    "            \n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            \n",
    "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
    "                \n",
    "                # One Hot Encode incoming data\n",
    "                x = one_hot_encoder(x,num_char)\n",
    "                \n",
    "\n",
    "                # Convert Numpy Arrays to Tensor\n",
    "\n",
    "                inputs = torch.from_numpy(x)\n",
    "                targets = torch.from_numpy(y)\n",
    "\n",
    "                # Adjust for GPU if necessary\n",
    "\n",
    "                if model.use_gpu:\n",
    "\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                    \n",
    "                # Reset Hidden State\n",
    "                # If we dont' reset we would backpropagate through \n",
    "                # all training history\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                \n",
    "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
    "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
    "        \n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Reset to training model after val for loop\n",
    "            model.train()\n",
    "            \n",
    "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pXH19P0PCBq"
   },
   "outputs": [],
   "source": [
    "model_name = 'lstm_on_shakespere.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "wClnXm-cQb-x",
    "outputId": "a6be8e7c-ad60-4022-a277-e9611d41a51f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f1bccaf8573f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8YDV78qQidK"
   },
   "outputs": [],
   "source": [
    "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
    "\n",
    "model = CharModel(\n",
    "    all_chars=all_characters,\n",
    "    num_hidden=512,\n",
    "    num_layers=3,\n",
    "    drop_prob=0.5,\n",
    "    use_gpu=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gS-1PXznQkRV",
    "outputId": "588dc3d4-e649-43bf-9cc2-1e4746d42ac1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(model_name))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EEND0kesQmE9"
   },
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "        \n",
    "        # Encode raw letters with model\n",
    "        encoded_text = model.encoder[char]\n",
    "        \n",
    "        # set as numpy array for one hot encoding\n",
    "        # NOTE THE [[ ]] dimensions!!\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        \n",
    "        # One hot encoding\n",
    "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "        \n",
    "        # Convert to Tensor\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        \n",
    "        # Check for CPU\n",
    "        if(model.use_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        \n",
    "        # Grab hidden states\n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        \n",
    "        \n",
    "        # Run model and get predicted output\n",
    "        lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "        \n",
    "        # Convert lstm_out to probabilities\n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        \n",
    "        \n",
    "        \n",
    "        if(model.use_gpu):\n",
    "            # move back to CPU to use with numpy\n",
    "            probs = probs.cpu()\n",
    "        \n",
    "        \n",
    "        # k determines how many characters to consider\n",
    "        # for our probability choice.\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "        \n",
    "        # Return k largest probabilities in tensor\n",
    "        probs, index_positions = probs.topk(k)\n",
    "        \n",
    "        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        \n",
    "        # Create array of probabilities\n",
    "        probs = probs.numpy().flatten()\n",
    "        \n",
    "        # Convert to probabilities per index\n",
    "        probs = probs/probs.sum()\n",
    "        \n",
    "        # randomly choose a character based on probabilities\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "       \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxH51sjnQrM9"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed='The', k=1):\n",
    "        \n",
    "      \n",
    "    \n",
    "    # CHECK FOR GPU\n",
    "    if(model.use_gpu):\n",
    "        model.cuda()\n",
    "    else:\n",
    "        model.cpu()\n",
    "    \n",
    "    # Evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # begin output from initial seed\n",
    "    output_chars = [c for c in seed]\n",
    "    \n",
    "    # intiate hidden state\n",
    "    hidden = model.hidden_state(1)\n",
    "    \n",
    "    # predict the next character for every character in seed\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "    \n",
    "    # add initial characters to output\n",
    "    output_chars.append(char)\n",
    "    \n",
    "    # Now generate for size requested\n",
    "    for i in range(size):\n",
    "        \n",
    "        # predict based off very last letter in output_chars\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "        \n",
    "        # add predicted character\n",
    "        output_chars.append(char)\n",
    "    \n",
    "    # return string of predicted text\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "n1hZtAegQtgQ",
    "outputId": "12baea7d-fe17-4bcc-ce11-e33007d87d09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word thou shalt be,\n",
      "    What there is such their serve to thee,\n",
      "    Then seal to see to him to have. If there's my\n",
      "    That I will be the story as him, and which you shall but\n",
      "    That true shall be true the better than his lease,\n",
      "    And then the wind our pleasure will.\n",
      "    The like this confess'd and such a man\n",
      "    That with this strong, when I am nature.\n",
      "    That they say shall then love them all the forest.\n",
      "    I will see my lord, and will see my son,\n",
      "    The such a womin's strange.  \n",
      "  AGRIPPA. And you shall be think of me; the man's fresh,\n",
      "    That the way's point, whither should have seen that\n",
      "    And true in the shows worth and such his\n",
      "    To be that thou art, and the sun that she\n",
      "    Than he hath beants to them at the way and\n",
      "    As that with me and the wine eyes we do not\n",
      "    To the string too marry.\n",
      "  ANTONY. I am no montill to him.\n",
      "    To the well way as think that I am good,\n",
      "    When they will seem her fortune have her speak,\n",
      "    Wherein he was and see their she take heavi\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The ', k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAEtX5GVQwOG"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP with PyTorch.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
