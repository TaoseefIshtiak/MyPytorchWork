{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP with PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5zUX45c7ll7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dip9L8KW7-yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('/content/shakespeare.txt','r',encoding='utf8') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUj65aaP8z2B",
        "colab_type": "code",
        "outputId": "96d48a31-aaa2-4556-e413-f019e5fd9fa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(text)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnGohlXL8FlG",
        "colab_type": "code",
        "outputId": "aec45827-1ecb-42c9-af31-d082c8a38182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFKVCyDaC7tt",
        "colab_type": "code",
        "outputId": "e08efb81-fef9-407d-fc7a-ece6d0e19370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(text) #no of characters in the entire text"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCT1IGBJDDO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_characters = set(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvDp7-psDFAB",
        "colab_type": "code",
        "outputId": "f312b750-98e2-41d6-b348-c9927bc78d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(all_characters)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HJPaJ1gDJre",
        "colab_type": "code",
        "outputId": "df5a84dc-c765-4786-cb0a-206c9d0d18f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for i in enumerate(all_characters):\n",
        "    print(i)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, ' ')\n",
            "(1, '.')\n",
            "(2, 'T')\n",
            "(3, 'k')\n",
            "(4, '1')\n",
            "(5, 'F')\n",
            "(6, 'D')\n",
            "(7, '0')\n",
            "(8, '7')\n",
            "(9, 'P')\n",
            "(10, ';')\n",
            "(11, 'f')\n",
            "(12, 'V')\n",
            "(13, '|')\n",
            "(14, 'd')\n",
            "(15, 'M')\n",
            "(16, 'h')\n",
            "(17, 'b')\n",
            "(18, ')')\n",
            "(19, 's')\n",
            "(20, '4')\n",
            "(21, 'K')\n",
            "(22, 'J')\n",
            "(23, 'X')\n",
            "(24, '<')\n",
            "(25, 'p')\n",
            "(26, 'L')\n",
            "(27, '6')\n",
            "(28, 'R')\n",
            "(29, '\\n')\n",
            "(30, '\"')\n",
            "(31, '3')\n",
            "(32, 'Y')\n",
            "(33, '8')\n",
            "(34, '}')\n",
            "(35, '2')\n",
            "(36, 'u')\n",
            "(37, '-')\n",
            "(38, '?')\n",
            "(39, 'A')\n",
            "(40, ']')\n",
            "(41, 'i')\n",
            "(42, 'a')\n",
            "(43, 'c')\n",
            "(44, ':')\n",
            "(45, 'W')\n",
            "(46, '5')\n",
            "(47, '>')\n",
            "(48, 'r')\n",
            "(49, '&')\n",
            "(50, '_')\n",
            "(51, 'O')\n",
            "(52, 'U')\n",
            "(53, 'e')\n",
            "(54, 'o')\n",
            "(55, \"'\")\n",
            "(56, 'H')\n",
            "(57, ',')\n",
            "(58, 'B')\n",
            "(59, 'z')\n",
            "(60, 'I')\n",
            "(61, 'l')\n",
            "(62, 'q')\n",
            "(63, '9')\n",
            "(64, 'w')\n",
            "(65, 'C')\n",
            "(66, '(')\n",
            "(67, 'E')\n",
            "(68, 'y')\n",
            "(69, 'Z')\n",
            "(70, 'x')\n",
            "(71, 't')\n",
            "(72, 'j')\n",
            "(73, '[')\n",
            "(74, '!')\n",
            "(75, '`')\n",
            "(76, 'N')\n",
            "(77, 'm')\n",
            "(78, 'g')\n",
            "(79, 'n')\n",
            "(80, 'Q')\n",
            "(81, 'G')\n",
            "(82, 'S')\n",
            "(83, 'v')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXZOuR7ZDLjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder = dict(enumerate(all_characters))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8J5yzj2kDOTy",
        "colab_type": "code",
        "outputId": "3d02ca0f-757c-4039-e378-10788705378b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "decoder"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ' ',\n",
              " 1: '.',\n",
              " 2: 'T',\n",
              " 3: 'k',\n",
              " 4: '1',\n",
              " 5: 'F',\n",
              " 6: 'D',\n",
              " 7: '0',\n",
              " 8: '7',\n",
              " 9: 'P',\n",
              " 10: ';',\n",
              " 11: 'f',\n",
              " 12: 'V',\n",
              " 13: '|',\n",
              " 14: 'd',\n",
              " 15: 'M',\n",
              " 16: 'h',\n",
              " 17: 'b',\n",
              " 18: ')',\n",
              " 19: 's',\n",
              " 20: '4',\n",
              " 21: 'K',\n",
              " 22: 'J',\n",
              " 23: 'X',\n",
              " 24: '<',\n",
              " 25: 'p',\n",
              " 26: 'L',\n",
              " 27: '6',\n",
              " 28: 'R',\n",
              " 29: '\\n',\n",
              " 30: '\"',\n",
              " 31: '3',\n",
              " 32: 'Y',\n",
              " 33: '8',\n",
              " 34: '}',\n",
              " 35: '2',\n",
              " 36: 'u',\n",
              " 37: '-',\n",
              " 38: '?',\n",
              " 39: 'A',\n",
              " 40: ']',\n",
              " 41: 'i',\n",
              " 42: 'a',\n",
              " 43: 'c',\n",
              " 44: ':',\n",
              " 45: 'W',\n",
              " 46: '5',\n",
              " 47: '>',\n",
              " 48: 'r',\n",
              " 49: '&',\n",
              " 50: '_',\n",
              " 51: 'O',\n",
              " 52: 'U',\n",
              " 53: 'e',\n",
              " 54: 'o',\n",
              " 55: \"'\",\n",
              " 56: 'H',\n",
              " 57: ',',\n",
              " 58: 'B',\n",
              " 59: 'z',\n",
              " 60: 'I',\n",
              " 61: 'l',\n",
              " 62: 'q',\n",
              " 63: '9',\n",
              " 64: 'w',\n",
              " 65: 'C',\n",
              " 66: '(',\n",
              " 67: 'E',\n",
              " 68: 'y',\n",
              " 69: 'Z',\n",
              " 70: 'x',\n",
              " 71: 't',\n",
              " 72: 'j',\n",
              " 73: '[',\n",
              " 74: '!',\n",
              " 75: '`',\n",
              " 76: 'N',\n",
              " 77: 'm',\n",
              " 78: 'g',\n",
              " 79: 'n',\n",
              " 80: 'Q',\n",
              " 81: 'G',\n",
              " 82: 'S',\n",
              " 83: 'v'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuT_oPazDQ4j",
        "colab_type": "code",
        "outputId": "73ae3fd4-b329-4c6d-d8bd-51982e921a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "decoder.items()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_items([(0, ' '), (1, '.'), (2, 'T'), (3, 'k'), (4, '1'), (5, 'F'), (6, 'D'), (7, '0'), (8, '7'), (9, 'P'), (10, ';'), (11, 'f'), (12, 'V'), (13, '|'), (14, 'd'), (15, 'M'), (16, 'h'), (17, 'b'), (18, ')'), (19, 's'), (20, '4'), (21, 'K'), (22, 'J'), (23, 'X'), (24, '<'), (25, 'p'), (26, 'L'), (27, '6'), (28, 'R'), (29, '\\n'), (30, '\"'), (31, '3'), (32, 'Y'), (33, '8'), (34, '}'), (35, '2'), (36, 'u'), (37, '-'), (38, '?'), (39, 'A'), (40, ']'), (41, 'i'), (42, 'a'), (43, 'c'), (44, ':'), (45, 'W'), (46, '5'), (47, '>'), (48, 'r'), (49, '&'), (50, '_'), (51, 'O'), (52, 'U'), (53, 'e'), (54, 'o'), (55, \"'\"), (56, 'H'), (57, ','), (58, 'B'), (59, 'z'), (60, 'I'), (61, 'l'), (62, 'q'), (63, '9'), (64, 'w'), (65, 'C'), (66, '('), (67, 'E'), (68, 'y'), (69, 'Z'), (70, 'x'), (71, 't'), (72, 'j'), (73, '['), (74, '!'), (75, '`'), (76, 'N'), (77, 'm'), (78, 'g'), (79, 'n'), (80, 'Q'), (81, 'G'), (82, 'S'), (83, 'v')])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP0qMOvFDTNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = {char: ind for ind,char in decoder.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0dyQNl5DWCN",
        "colab_type": "code",
        "outputId": "881ca9b0-2e77-490e-faf1-557d94e922ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "encoder"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 29,\n",
              " ' ': 0,\n",
              " '!': 74,\n",
              " '\"': 30,\n",
              " '&': 49,\n",
              " \"'\": 55,\n",
              " '(': 66,\n",
              " ')': 18,\n",
              " ',': 57,\n",
              " '-': 37,\n",
              " '.': 1,\n",
              " '0': 7,\n",
              " '1': 4,\n",
              " '2': 35,\n",
              " '3': 31,\n",
              " '4': 20,\n",
              " '5': 46,\n",
              " '6': 27,\n",
              " '7': 8,\n",
              " '8': 33,\n",
              " '9': 63,\n",
              " ':': 44,\n",
              " ';': 10,\n",
              " '<': 24,\n",
              " '>': 47,\n",
              " '?': 38,\n",
              " 'A': 39,\n",
              " 'B': 58,\n",
              " 'C': 65,\n",
              " 'D': 6,\n",
              " 'E': 67,\n",
              " 'F': 5,\n",
              " 'G': 81,\n",
              " 'H': 56,\n",
              " 'I': 60,\n",
              " 'J': 22,\n",
              " 'K': 21,\n",
              " 'L': 26,\n",
              " 'M': 15,\n",
              " 'N': 76,\n",
              " 'O': 51,\n",
              " 'P': 9,\n",
              " 'Q': 80,\n",
              " 'R': 28,\n",
              " 'S': 82,\n",
              " 'T': 2,\n",
              " 'U': 52,\n",
              " 'V': 12,\n",
              " 'W': 45,\n",
              " 'X': 23,\n",
              " 'Y': 32,\n",
              " 'Z': 69,\n",
              " '[': 73,\n",
              " ']': 40,\n",
              " '_': 50,\n",
              " '`': 75,\n",
              " 'a': 42,\n",
              " 'b': 17,\n",
              " 'c': 43,\n",
              " 'd': 14,\n",
              " 'e': 53,\n",
              " 'f': 11,\n",
              " 'g': 78,\n",
              " 'h': 16,\n",
              " 'i': 41,\n",
              " 'j': 72,\n",
              " 'k': 3,\n",
              " 'l': 61,\n",
              " 'm': 77,\n",
              " 'n': 79,\n",
              " 'o': 54,\n",
              " 'p': 25,\n",
              " 'q': 62,\n",
              " 'r': 48,\n",
              " 's': 19,\n",
              " 't': 71,\n",
              " 'u': 36,\n",
              " 'v': 83,\n",
              " 'w': 64,\n",
              " 'x': 70,\n",
              " 'y': 68,\n",
              " 'z': 59,\n",
              " '|': 13,\n",
              " '}': 34}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXVF-8hUDXv_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKFD9NKzDaRK",
        "colab_type": "code",
        "outputId": "5b741f02-b62b-4207-9eb8-05e38bc1e1bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "encoded_text[:500]"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0,  0,  0,  4, 29,  0,  0,  5, 48, 54, 77,  0, 11, 42, 41,\n",
              "       48, 53, 19, 71,  0, 43, 48, 53, 42, 71, 36, 48, 53, 19,  0, 64, 53,\n",
              "        0, 14, 53, 19, 41, 48, 53,  0, 41, 79, 43, 48, 53, 42, 19, 53, 57,\n",
              "       29,  0,  0,  2, 16, 42, 71,  0, 71, 16, 53, 48, 53, 17, 68,  0, 17,\n",
              "       53, 42, 36, 71, 68, 55, 19,  0, 48, 54, 19, 53,  0, 77, 41, 78, 16,\n",
              "       71,  0, 79, 53, 83, 53, 48,  0, 14, 41, 53, 57, 29,  0,  0, 58, 36,\n",
              "       71,  0, 42, 19,  0, 71, 16, 53,  0, 48, 41, 25, 53, 48,  0, 19, 16,\n",
              "       54, 36, 61, 14,  0, 17, 68,  0, 71, 41, 77, 53,  0, 14, 53, 43, 53,\n",
              "       42, 19, 53, 57, 29,  0,  0, 56, 41, 19,  0, 71, 53, 79, 14, 53, 48,\n",
              "        0, 16, 53, 41, 48,  0, 77, 41, 78, 16, 71,  0, 17, 53, 42, 48,  0,\n",
              "       16, 41, 19,  0, 77, 53, 77, 54, 48, 68, 44, 29,  0,  0, 58, 36, 71,\n",
              "        0, 71, 16, 54, 36,  0, 43, 54, 79, 71, 48, 42, 43, 71, 53, 14,  0,\n",
              "       71, 54,  0, 71, 16, 41, 79, 53,  0, 54, 64, 79,  0, 17, 48, 41, 78,\n",
              "       16, 71,  0, 53, 68, 53, 19, 57, 29,  0,  0,  5, 53, 53, 14, 55, 19,\n",
              "       71,  0, 71, 16, 68,  0, 61, 41, 78, 16, 71, 55, 19,  0, 11, 61, 42,\n",
              "       77, 53,  0, 64, 41, 71, 16,  0, 19, 53, 61, 11, 37, 19, 36, 17, 19,\n",
              "       71, 42, 79, 71, 41, 42, 61,  0, 11, 36, 53, 61, 57, 29,  0,  0, 15,\n",
              "       42,  3, 41, 79, 78,  0, 42,  0, 11, 42, 77, 41, 79, 53,  0, 64, 16,\n",
              "       53, 48, 53,  0, 42, 17, 36, 79, 14, 42, 79, 43, 53,  0, 61, 41, 53,\n",
              "       19, 57, 29,  0,  0,  2, 16, 68,  0, 19, 53, 61, 11,  0, 71, 16, 68,\n",
              "        0, 11, 54, 53, 57,  0, 71, 54,  0, 71, 16, 68,  0, 19, 64, 53, 53,\n",
              "       71,  0, 19, 53, 61, 11,  0, 71, 54, 54,  0, 43, 48, 36, 53, 61, 44,\n",
              "       29,  0,  0,  2, 16, 54, 36,  0, 71, 16, 42, 71,  0, 42, 48, 71,  0,\n",
              "       79, 54, 64,  0, 71, 16, 53,  0, 64, 54, 48, 61, 14, 55, 19,  0, 11,\n",
              "       48, 53, 19, 16,  0, 54, 48, 79, 42, 77, 53, 79, 71, 57, 29,  0,  0,\n",
              "       39, 79, 14,  0, 54, 79, 61, 68,  0, 16, 53, 48, 42, 61, 14,  0, 71,\n",
              "       54,  0, 71, 16, 53,  0, 78, 42, 36, 14, 68,  0, 19, 25, 48, 41, 79,\n",
              "       78, 57, 29,  0,  0, 45, 41, 71, 16, 41, 79,  0, 71, 16, 41, 79, 53,\n",
              "        0, 54, 64, 79,  0, 17, 36])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZU5lL8nkDgIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_hot_encoder(encoded_text, num_uni_chars):\n",
        "    '''\n",
        "    encoded_text : batch of encoded text\n",
        "    \n",
        "    num_uni_chars = number of unique characters (len(set(text)))\n",
        "    '''\n",
        "    \n",
        "    # method obtained from:\n",
        "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
        "      \n",
        "    # creating a placeholder for zeros.\n",
        "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
        "    \n",
        "    # converting data type for later use with pytorch (errors if we dont!)\n",
        "    one_hot = one_hot.astype(np.float32)\n",
        "\n",
        "    # using fancy indexing fill in the 1s at the correct index locations\n",
        "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "    \n",
        "\n",
        "    # reshaping it so it matches the batch sahe\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B44kB9IIDi_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#small example of one hot encoding\n",
        "arr = np.array([1,2,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cZivY1MDlik",
        "colab_type": "code",
        "outputId": "e7e91e67-4188-4fb3-c081-b99315224c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "arr"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96Ros5CDDnDr",
        "colab_type": "code",
        "outputId": "0c84d14e-e091-40b7-f18d-fcea6cff1a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "one_hot_encoder(arr, 3)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFWtbH9pDs-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
        "    \n",
        "    '''\n",
        "    Generating (using yield) batches for training.\n",
        "    \n",
        "    X: Encoded Text of length seq_len\n",
        "    Y: Encoded Text shifted by one\n",
        "    Example:\n",
        "    X:\n",
        "    [[1 2 3]]\n",
        "    \n",
        "    Y:\n",
        "    [[ 2 3 4]]\n",
        "    encoded_text : Completing Encoded Text to make batches from batch_size : Number of samples per batch seq_len : \n",
        "    Length of character sequence\n",
        "       \n",
        "    '''\n",
        "    # Total number of characters per batch\n",
        "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100  characters come out per batch.\n",
        "    char_per_batch = samp_per_batch * seq_len\n",
        "    \n",
        "    \n",
        "    # Number of batches available to make\n",
        "    # using int() to roun to nearest integer\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "    \n",
        "    # cutting off end of encoded_text that\n",
        "    # won't fit evenly into a batch\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
        "    \n",
        "    \n",
        "    # Reshape text into rows the size of a batch\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "    \n",
        "\n",
        "    # Go through each row in array.\n",
        "    for n in range(0, encoded_text.shape[1], seq_len):\n",
        "        \n",
        "        # Grab feature characters\n",
        "        x = encoded_text[:, n:n+seq_len]\n",
        "        \n",
        "        # y is the target shifted over by 1\n",
        "        y = np.zeros_like(x)\n",
        "       \n",
        "        #\n",
        "        try:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "            \n",
        "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
        "        except:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "            \n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfjSMAzEDwlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_text = encoded_text[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkUO4suHDy46",
        "colab_type": "code",
        "outputId": "12d7705a-6858-41f2-8e1f-d57e446117e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_text"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([29,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
              "        0,  0,  0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMn32rTED09_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_generator = generate_batches(sample_text,samp_per_batch=2,seq_len=5) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KuFmW9r_D3Zb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y  = next(batch_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuWk_YkxD59s",
        "colab_type": "code",
        "outputId": "49f3f867-c86a-4ee2-ea72-de9f25f252f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[29,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_WrIy_PD8pi",
        "colab_type": "code",
        "outputId": "904ae0e9-19ec-4d4a-cd82-8af352959474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "y"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgY_EYpiD_Xh",
        "colab_type": "code",
        "outputId": "2280e151-8d3a-4719-f078-03d98c34fda3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErHYcFk9KYKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_cuda = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfZH9AsEKbZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CharModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "        \n",
        "        \n",
        "        # setting up attributes\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        #CHARACTER SET, ENCODER, and DECODER\n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars))\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "        \n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                  \n",
        "        \n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        \n",
        "        drop_output = self.dropout(lstm_output)\n",
        "        \n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "        \n",
        "        \n",
        "        final_out = self.fc_linear(drop_output)\n",
        "        \n",
        "        \n",
        "        return final_out, hidden\n",
        "    \n",
        "    \n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "        \n",
        "        if self.use_gpu:\n",
        "            \n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIm-URP0N6Qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDSp_xwYOAMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_param  = []\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKVsmlXqOhNt",
        "colab_type": "code",
        "outputId": "57402d36-3f54-4197-fd54-86df66cfdaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sum(total_param)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnNLUI0VOjfk",
        "colab_type": "code",
        "outputId": "7cf61341-59c6-4808-cbfd-4ff60fc7879d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(encoded_text)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUYiANhbOlme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmu6OngUOn30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# percentage of data to be used for training\n",
        "train_percent = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCXZSpH9Opg-",
        "colab_type": "code",
        "outputId": "9f1b1227-1fc4-4d8c-c278-7957d2cb7bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "int(len(encoded_text) * (train_percent))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74RPqFovOsO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ind = int(len(encoded_text) * (train_percent))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz6Pd6bKOuH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IyW-qnPGOwqW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Epochs to train for\n",
        "epochs = 50\n",
        "# batch size \n",
        "batch_size = 128\n",
        "\n",
        "# Length of sequence\n",
        "seq_len = 100\n",
        "\n",
        "# for printing report purposes\n",
        "# always start at 0\n",
        "tracker = 0\n",
        "\n",
        "# number of characters in text\n",
        "num_char = max(encoded_text)+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8yaKrmMO8yW",
        "colab_type": "code",
        "outputId": "72a5662c-8f82-43cc-cf01-701f6f3b2b95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Set model to train\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "    hidden = model.hidden_state(batch_size)\n",
        "    \n",
        "    \n",
        "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "        \n",
        "        tracker += 1\n",
        "        \n",
        "        # One Hot Encode incoming data\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "        \n",
        "        # Convert Numpy Arrays to Tensor\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "        \n",
        "        # Adjust for GPU if necessary\n",
        "        \n",
        "        if model.use_gpu:\n",
        "            \n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "            \n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        ###################################\n",
        "        ### CHECK ON VALIDATION SET ######\n",
        "        #################################\n",
        "        \n",
        "        if tracker % 25 == 0:\n",
        "            \n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            \n",
        "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "                \n",
        "                # One Hot Encode incoming data\n",
        "                x = one_hot_encoder(x,num_char)\n",
        "                \n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "                    \n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through \n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "                \n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "            \n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.237664222717285\n",
            "Epoch: 1 Step: 50 Val Loss: 3.2331621646881104\n",
            "Epoch: 1 Step: 75 Val Loss: 3.2267045974731445\n",
            "Epoch: 2 Step: 100 Val Loss: 3.089893102645874\n",
            "Epoch: 2 Step: 125 Val Loss: 2.998598337173462\n",
            "Epoch: 3 Step: 150 Val Loss: 2.849747896194458\n",
            "Epoch: 4 Step: 175 Val Loss: 2.766860008239746\n",
            "Epoch: 4 Step: 200 Val Loss: 2.6719610691070557\n",
            "Epoch: 5 Step: 225 Val Loss: 2.548754930496216\n",
            "Epoch: 5 Step: 250 Val Loss: 2.4534506797790527\n",
            "Epoch: 6 Step: 275 Val Loss: 2.3802664279937744\n",
            "Epoch: 7 Step: 300 Val Loss: 2.3201465606689453\n",
            "Epoch: 7 Step: 325 Val Loss: 2.2700226306915283\n",
            "Epoch: 8 Step: 350 Val Loss: 2.2233948707580566\n",
            "Epoch: 8 Step: 375 Val Loss: 2.1848795413970947\n",
            "Epoch: 9 Step: 400 Val Loss: 2.1455702781677246\n",
            "Epoch: 10 Step: 425 Val Loss: 2.119403600692749\n",
            "Epoch: 10 Step: 450 Val Loss: 2.089174270629883\n",
            "Epoch: 11 Step: 475 Val Loss: 2.0637459754943848\n",
            "Epoch: 11 Step: 500 Val Loss: 2.0452277660369873\n",
            "Epoch: 12 Step: 525 Val Loss: 2.0246803760528564\n",
            "Epoch: 13 Step: 550 Val Loss: 2.010317087173462\n",
            "Epoch: 13 Step: 575 Val Loss: 1.9859548807144165\n",
            "Epoch: 14 Step: 600 Val Loss: 1.9698137044906616\n",
            "Epoch: 14 Step: 625 Val Loss: 1.9487502574920654\n",
            "Epoch: 15 Step: 650 Val Loss: 1.942155361175537\n",
            "Epoch: 16 Step: 675 Val Loss: 1.9257493019104004\n",
            "Epoch: 16 Step: 700 Val Loss: 1.9123426675796509\n",
            "Epoch: 17 Step: 725 Val Loss: 1.8913390636444092\n",
            "Epoch: 17 Step: 750 Val Loss: 1.8869960308074951\n",
            "Epoch: 18 Step: 775 Val Loss: 1.8784561157226562\n",
            "Epoch: 19 Step: 800 Val Loss: 1.865846872329712\n",
            "Epoch: 19 Step: 825 Val Loss: 1.8563742637634277\n",
            "Epoch: 20 Step: 850 Val Loss: 1.8462393283843994\n",
            "Epoch: 20 Step: 875 Val Loss: 1.837915301322937\n",
            "Epoch: 21 Step: 900 Val Loss: 1.8338857889175415\n",
            "Epoch: 22 Step: 925 Val Loss: 1.8252949714660645\n",
            "Epoch: 22 Step: 950 Val Loss: 1.8229633569717407\n",
            "Epoch: 23 Step: 975 Val Loss: 1.8128154277801514\n",
            "Epoch: 23 Step: 1000 Val Loss: 1.810305118560791\n",
            "Epoch: 24 Step: 1025 Val Loss: 1.8033784627914429\n",
            "Epoch: 24 Step: 1050 Val Loss: 1.7973982095718384\n",
            "Epoch: 25 Step: 1075 Val Loss: 1.8037513494491577\n",
            "Epoch: 26 Step: 1100 Val Loss: 1.789353370666504\n",
            "Epoch: 26 Step: 1125 Val Loss: 1.7845101356506348\n",
            "Epoch: 27 Step: 1150 Val Loss: 1.7776566743850708\n",
            "Epoch: 27 Step: 1175 Val Loss: 1.779240608215332\n",
            "Epoch: 28 Step: 1200 Val Loss: 1.7759428024291992\n",
            "Epoch: 29 Step: 1225 Val Loss: 1.7714130878448486\n",
            "Epoch: 29 Step: 1250 Val Loss: 1.7649050951004028\n",
            "Epoch: 30 Step: 1275 Val Loss: 1.7573391199111938\n",
            "Epoch: 30 Step: 1300 Val Loss: 1.763718843460083\n",
            "Epoch: 31 Step: 1325 Val Loss: 1.7558668851852417\n",
            "Epoch: 32 Step: 1350 Val Loss: 1.7544599771499634\n",
            "Epoch: 32 Step: 1375 Val Loss: 1.7608453035354614\n",
            "Epoch: 33 Step: 1400 Val Loss: 1.7483552694320679\n",
            "Epoch: 33 Step: 1425 Val Loss: 1.7468615770339966\n",
            "Epoch: 34 Step: 1450 Val Loss: 1.7458863258361816\n",
            "Epoch: 35 Step: 1475 Val Loss: 1.7411099672317505\n",
            "Epoch: 35 Step: 1500 Val Loss: 1.7396374940872192\n",
            "Epoch: 36 Step: 1525 Val Loss: 1.736566424369812\n",
            "Epoch: 36 Step: 1550 Val Loss: 1.7353814840316772\n",
            "Epoch: 37 Step: 1575 Val Loss: 1.7406036853790283\n",
            "Epoch: 38 Step: 1600 Val Loss: 1.729941725730896\n",
            "Epoch: 38 Step: 1625 Val Loss: 1.732100486755371\n",
            "Epoch: 39 Step: 1650 Val Loss: 1.729580044746399\n",
            "Epoch: 39 Step: 1675 Val Loss: 1.7289634943008423\n",
            "Epoch: 40 Step: 1700 Val Loss: 1.7320969104766846\n",
            "Epoch: 41 Step: 1725 Val Loss: 1.7275689840316772\n",
            "Epoch: 41 Step: 1750 Val Loss: 1.7252403497695923\n",
            "Epoch: 42 Step: 1775 Val Loss: 1.7352389097213745\n",
            "Epoch: 42 Step: 1800 Val Loss: 1.7387787103652954\n",
            "Epoch: 43 Step: 1825 Val Loss: 1.7273160219192505\n",
            "Epoch: 44 Step: 1850 Val Loss: 1.7292492389678955\n",
            "Epoch: 44 Step: 1875 Val Loss: 1.7250337600708008\n",
            "Epoch: 45 Step: 1900 Val Loss: 1.729353904724121\n",
            "Epoch: 45 Step: 1925 Val Loss: 1.7247471809387207\n",
            "Epoch: 46 Step: 1950 Val Loss: 1.725437879562378\n",
            "Epoch: 47 Step: 1975 Val Loss: 1.7310442924499512\n",
            "Epoch: 47 Step: 2000 Val Loss: 1.7382214069366455\n",
            "Epoch: 48 Step: 2025 Val Loss: 1.7405366897583008\n",
            "Epoch: 48 Step: 2050 Val Loss: 1.7302912473678589\n",
            "Epoch: 49 Step: 2075 Val Loss: 1.7271308898925781\n",
            "Epoch: 49 Step: 2100 Val Loss: 1.7312580347061157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pXH19P0PCBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'lstm_on_shakespere.net'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wClnXm-cQb-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(),model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8YDV78qQidK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
        "\n",
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gS-1PXznQkRV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6b743d3f-ba55-41ca-cd7a-6246cf63c5be"
      },
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(83, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=83, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEND0kesQmE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "        \n",
        "        # Encode raw letters with model\n",
        "        encoded_text = model.encoder[char]\n",
        "        \n",
        "        # set as numpy array for one hot encoding\n",
        "        # NOTE THE [[ ]] dimensions!!\n",
        "        encoded_text = np.array([[encoded_text]])\n",
        "        \n",
        "        # One hot encoding\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "        \n",
        "        # Convert to Tensor\n",
        "        inputs = torch.from_numpy(encoded_text)\n",
        "        \n",
        "        # Check for CPU\n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        \n",
        "        # Grab hidden states\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        \n",
        "        # Run model and get predicted output\n",
        "        lstm_out, hidden = model(inputs, hidden)\n",
        "\n",
        "        \n",
        "        # Convert lstm_out to probabilities\n",
        "        probs = F.softmax(lstm_out, dim=1).data\n",
        "        \n",
        "        \n",
        "        \n",
        "        if(model.use_gpu):\n",
        "            # move back to CPU to use with numpy\n",
        "            probs = probs.cpu()\n",
        "        \n",
        "        \n",
        "        # k determines how many characters to consider\n",
        "        # for our probability choice.\n",
        "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
        "        \n",
        "        # Return k largest probabilities in tensor\n",
        "        probs, index_positions = probs.topk(k)\n",
        "        \n",
        "        \n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "        \n",
        "        # Create array of probabilities\n",
        "        probs = probs.numpy().flatten()\n",
        "        \n",
        "        # Convert to probabilities per index\n",
        "        probs = probs/probs.sum()\n",
        "        \n",
        "        # randomly choose a character based on probabilities\n",
        "        char = np.random.choice(index_positions, p=probs)\n",
        "       \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.decoder[char], hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxH51sjnQrM9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "        \n",
        "      \n",
        "    \n",
        "    # CHECK FOR GPU\n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "    \n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # begin output from initial seed\n",
        "    output_chars = [c for c in seed]\n",
        "    \n",
        "    # intiate hidden state\n",
        "    hidden = model.hidden_state(1)\n",
        "    \n",
        "    # predict the next character for every character in seed\n",
        "    for char in seed:\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "    \n",
        "    # add initial characters to output\n",
        "    output_chars.append(char)\n",
        "    \n",
        "    # Now generate for size requested\n",
        "    for i in range(size):\n",
        "        \n",
        "        # predict based off very last letter in output_chars\n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "        \n",
        "        # add predicted character\n",
        "        output_chars.append(char)\n",
        "    \n",
        "    # return string of predicted text\n",
        "    return ''.join(output_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1hZtAegQtgQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "c1651fc7-1181-42d0-b478-1201597ab719"
      },
      "source": [
        "print(generate_text(model, 1000, seed='The ', k=3))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The stand to-this\n",
            "    And my live of the will to-be their loves,\n",
            "    That hear the wind the wind of my beauty strangies.\n",
            "    In him in many, then that heaven of them stand.\n",
            "    That when the senses and stand, which thoush the strangth\n",
            "    When he will the strange of the suncine of this count.\n",
            "\n",
            "                             Enter PAROLLES\n",
            "\n",
            "  LAFEU. Which the streeks and stard of the sides of thy self.\n",
            "\n",
            "                            Enter ANTONY\n",
            "\n",
            "    I would no sold in thy honour to her,  \n",
            "    Which thou sholld't hate and see a serve to me.\n",
            "\n",
            "\n",
            "                     111\n",
            "  That to thy beauten see that sen too lears,\n",
            "  Which somethow beauty thee which the will,\n",
            "  We store thee winds to serve the store,\n",
            "  Which they wishon to thy love to mean,\n",
            "  And that they seem that they be seen to thee,\n",
            "  The store one of a maid of this sold,\n",
            "  And such to me that stood to the treep,\n",
            "  The both which some thou shall store see they,\n",
            "  The stores of her shand be so such to store.\n",
            "  Or to the wander which thou things\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAEtX5GVQwOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}